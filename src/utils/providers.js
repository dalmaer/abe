import { generateText, experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google, createGoogleGenerativeAI } from '@ai-sdk/google';
import fs from 'fs-extra';

const MAX_RETRIES = 3;
const RETRY_DELAY = 1000;

export async function callImageModel({ 
  providerModel, 
  prompt, 
  imageInput, 
  seed,
  config,
  maxRetries = 3
}) {
  const [provider, model] = parseProviderModel(providerModel);
  
  let lastError;
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      await delay(attempt * RETRY_DELAY);
      
      if (isImageGenerationModel(providerModel)) {
        // Special handling for Gemini Flash image preview models
        if (provider === 'google' && model.includes('gemini')) {
          const geminiModel = getModelConfig(provider, model, config);
          
          const result = await generateText({
            model: geminiModel,
            prompt,
            providerOptions: {
              google: { responseModalities: ['TEXT', 'IMAGE'] }
            }
          });
          
          // Find the first image file in the response
          const imageFile = result.files?.find(file => file.mediaType?.startsWith('image/'));
          if (!imageFile) {
            throw new Error('No image generated by Gemini model');
          }
          
          
          // Convert the file to buffer format expected by the rest of the system
          let imageBuffer;
          if (imageFile.url) {
            const response = await fetch(imageFile.url);
            imageBuffer = Buffer.from(await response.arrayBuffer());
          } else if (imageFile.data) {
            imageBuffer = Buffer.from(imageFile.data);
          } else if (imageFile.base64) {
            imageBuffer = Buffer.from(imageFile.base64, 'base64');
          } else if (imageFile.base64Data) {
            imageBuffer = Buffer.from(imageFile.base64Data, 'base64');
          } else {
            throw new Error(`No compatible image data format in Gemini response. Available properties: ${Object.keys(imageFile).join(', ')}`);
          }
          
          return {
            image: imageBuffer,
            metadata: {
              provider,
              model,
              prompt,
              seed,
              timestamp: new Date().toISOString(),
              geminiText: result.text // Include any text response
            }
          };
        } else {
          // Use regular image generation for other models (Imagen, DALL-E, etc.)
          const imageModel = getImageModel(provider, model, config);
          
          const generateOptions = {
            model: imageModel,
            prompt,
            n: 1,
            size: '1024x1024',
            ...(seed ? { seed } : {})
          };
          
          // Add Google-specific options for Imagen
          if (provider === 'google') {
            delete generateOptions.size;
            generateOptions.aspectRatio = '1:1';
            generateOptions.providerOptions = {
              google: { responseModalities: ['IMAGE'] }
            };
          }
          
          const result = await generateImage(generateOptions);
          
          // Get the first generated image - handle both images array and files array
          const image = result.images?.[0] || result.files?.[0];
        
        if (!image) {
          throw new Error('No image data received from provider');
        }
        
        // Convert image to buffer if it's a URL or base64
        let imageBuffer;
        if (image.url) {
          // Fetch image from URL
          const response = await fetch(image.url);
          imageBuffer = Buffer.from(await response.arrayBuffer());
        } else if (image.base64) {
          imageBuffer = Buffer.from(image.base64, 'base64');
        } else if (image.data) {
          // Some providers might return raw data
          imageBuffer = Buffer.from(image.data);
        } else {
          throw new Error('No compatible image data format received');
        }
        
          return {
            image: imageBuffer,
            metadata: {
              provider,
              model,
              prompt,
              seed,
              size: '1024x1024',
              timestamp: new Date().toISOString()
            }
          };
        }
      } else {
        // Fall back to text description for non-image models
        const modelConfig = getModelConfig(provider, model, config);
        
        const result = await generateText({
          model: modelConfig,
          prompt: `Generate a detailed design description for this UI mockup request: ${prompt}`
        });
        
        return {
          image: Buffer.from(`Text Description: ${result.text}`),
          metadata: {
            provider,
            model,
            prompt,
            seed,
            description: result.text,
            timestamp: new Date().toISOString(),
            isTextDescription: true
          }
        };
      }
    } catch (error) {
      lastError = error;
      console.warn(`Attempt ${attempt + 1} failed for ${providerModel}:`, error.message);
      
      if (attempt === MAX_RETRIES - 1) {
        throw new Error(`Failed to generate image after ${MAX_RETRIES} attempts: ${lastError.message}`);
      }
    }
  }
}

export async function callVisionCritic({ 
  providerModel, 
  prompt, 
  imagePath,
  config 
}) {
  const [provider, model] = parseProviderModel(providerModel);
  
  let lastError;
  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      await delay(attempt * RETRY_DELAY);
      
      const modelConfig = getModelConfig(provider, model, config);
      
      let imageData;
      if (typeof imagePath === 'string') {
        imageData = await fs.readFile(imagePath);
      } else {
        imageData = imagePath;
      }
      
      const result = await generateText({
        model: modelConfig,
        messages: [
          {
            role: 'user',
            content: [
              { type: 'text', text: prompt },
              {
                type: 'image',
                image: imageData
              }
            ]
          }
        ]
      });
      
      return {
        text: result.text,
        metadata: {
          provider,
          model,
          timestamp: new Date().toISOString()
        }
      };
    } catch (error) {
      lastError = error;
      console.warn(`Attempt ${attempt + 1} failed for ${providerModel}:`, error.message);
      
      if (attempt === MAX_RETRIES - 1) {
        throw new Error(`Failed to critique image after ${MAX_RETRIES} attempts: ${lastError.message}`);
      }
    }
  }
}

export async function callTextModel({ 
  providerModel, 
  prompt,
  config 
}) {
  const [provider, model] = parseProviderModel(providerModel);
  
  let lastError;
  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      await delay(attempt * RETRY_DELAY);
      
      const modelConfig = getModelConfig(provider, model, config);
      
      const result = await generateText({
        model: modelConfig,
        prompt
      });
      
      return {
        text: result.text,
        metadata: {
          provider,
          model,
          timestamp: new Date().toISOString()
        }
      };
    } catch (error) {
      lastError = error;
      console.warn(`Attempt ${attempt + 1} failed for ${providerModel}:`, error.message);
      
      if (attempt === MAX_RETRIES - 1) {
        throw new Error(`Failed to call text model after ${MAX_RETRIES} attempts: ${lastError.message}`);
      }
    }
  }
}

function parseProviderModel(providerModel) {
  const parts = providerModel.split(':');
  if (parts.length !== 2) {
    throw new Error(`Invalid provider:model format: ${providerModel}`);
  }
  return parts;
}

function getModelConfig(provider, model, config) {
  const providerOptions = config?.providerOptions?.[provider] || {};
  
  switch (provider) {
    case 'openai':
      return openai(model, {
        apiKey: process.env[providerOptions.apiKeyEnv || 'OPENAI_API_KEY'],
        ...providerOptions
      });
    
    case 'anthropic':
      return anthropic(model, {
        apiKey: process.env[providerOptions.apiKeyEnv || 'ANTHROPIC_API_KEY'],
        ...providerOptions
      });
      
    case 'google':
      const googleProvider = createGoogleGenerativeAI({
        apiKey: process.env[providerOptions.apiKeyEnv || 'GEMINI_API_KEY']
      });
      return googleProvider(model);
    
    default:
      throw new Error(`Unsupported provider: ${provider}`);
  }
}

function getImageModel(provider, model, config) {
  const providerOptions = config?.providerOptions?.[provider] || {};
  
  switch (provider) {
    case 'openai':
      // Map to actual image generation models
      const imageModelName = model === 'gpt-4o' ? 'dall-e-3' : model;
      return openai(imageModelName, {
        apiKey: process.env[providerOptions.apiKeyEnv || 'OPENAI_API_KEY'],
        ...providerOptions
      });
      
    case 'google':
      // Use Google's image generation model with explicit API key configuration
      const apiKey = process.env[providerOptions.apiKeyEnv || 'GEMINI_API_KEY'];
      const googleImageModel = model.includes('gemini') ? model : 'imagen-3.0-generate-002';
      const googleProvider = createGoogleGenerativeAI({
        apiKey: apiKey
      });
      // Use .image() for image generation models
      if (model.includes('imagen')) {
        return googleProvider.image(googleImageModel);
      } else if (model.includes('gemini')) {
        // For Gemini models, use regular text generation model 
        // (they handle image generation through generateImage API)
        return googleProvider(googleImageModel);
      } else {
        return googleProvider(googleImageModel);
      }
    
    default:
      throw new Error(`Image generation not supported for provider: ${provider}`);
  }
}

function delay(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

export function isImageGenerationModel(providerModel) {
  const imageModels = [
    'openai:dall-e-3',
    'openai:dall-e-2',
    'openai:gpt-4o', // We'll map this to dall-e-3
    'google:gemini-2.5-flash-image-preview',
    'google:imagen-3.0-generate-002',
    'google:imagen-3',
    'stability:sd3-medium',
    'stability:sd3-large'
  ];
  
  return imageModels.includes(providerModel) ||
         providerModel.includes('dall-e') ||
         providerModel.includes('gemini') ||
         providerModel.includes('sd3') ||
         providerModel.includes('imagen');
}

export function isVisionModel(providerModel) {
  const visionModels = [
    'openai:gpt-4o',
    'openai:gpt-4-turbo',
    'anthropic:claude-3-5-sonnet-20241022',
    'anthropic:claude-3-opus-20240229',
    'anthropic:claude-3-haiku-20240307',
    'google:gemini-pro-vision'
  ];
  
  return visionModels.some(model => 
    providerModel.includes(model.split(':')[1]) || 
    providerModel === model
  );
}